{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Zodiac_Sings_Image_Classification_(CNN_&_Transfer_Learning)_PyTorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gstdl/Brazilian_e-Commerce_Exploartory_Data_Analysis_and_Machine_Learning/blob/master/Zodiac_Sings_Image_Classification_(CNN_%26_Transfer_Learning)_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdaY2AhPgx9k",
        "colab_type": "text"
      },
      "source": [
        "# Intro\n",
        "## Dataset Download\n",
        "\n",
        "The dataset is available in [Google Drive](https://drive.google.com/file/d/1Bjope31ZX9p4jXyaYK6CazIOGcYhOjI6/view?usp=sharing) and [Kaggle](https://www.kaggle.com/elderyouth/chinese-zodiac-signs). For my convenience, I prefer downloading the dataset directly from Kaggle to Google Colab. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwno3L4_CRLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.files import upload\n",
        "upload = upload() # Upload Kaggle API Key"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O962MoOV3ecE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kaggle # Installs Kaggle\n",
        "!mkdir kaggle # Makes new directory 'kaggle'\n",
        "!mkdir ../root/.kaggle # Makes new directory '../root/.kaggle'\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json # Copies kaggle API key to the new directory\n",
        "!kaggle config set -n path -v{/content}\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d elderyouth/chinese-zodiac-signs # Downloads the dataset as a zip file\n",
        "!unzip /content/{/content}/datasets/elderyouth/chinese-zodiac-signs/chinese-zodiac-signs.zip # Unzips the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VrkB_kNhlm9",
        "colab_type": "text"
      },
      "source": [
        "**Early Debugging**\n",
        "\n",
        "During my initial training I had several problems. The code below took care of the problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANQn-7HlxyHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True # Allows the image loader to read truncated images\n",
        "\n",
        "to_delete = [\n",
        "             'signs/train/snake/00000116.jpg',\n",
        "             'signs/train/monkey/00000585.jpg'\n",
        "] # Problematic files\n",
        "\n",
        "import os\n",
        "for delete in to_delete: \n",
        "    os.remove(delete) # Delete problematic files\n",
        "    print('deleting: ',delete)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGouTVYFkgqC",
        "colab_type": "text"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVEoHsA93IUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT1RLt6nq4aR",
        "colab_type": "text"
      },
      "source": [
        "**Check GPU availability**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovXm1esXt3Kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6oNx7WlLgFv",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning\n",
        "## Developing my own Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpDav2Cv3IUi",
        "colab_type": "text"
      },
      "source": [
        "## Transformations and Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxXr-kLk3IUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'signs'\n",
        "\n",
        "batch_size = 20\n",
        "\n",
        "# TODO: Define transforms for the training data and testing data\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.5, 0.5, 0.5],\n",
        "                                                           [0.5, 0.5, 0.5])])\n",
        "\n",
        "test_valid_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.RandomResizedCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.5, 0.5, 0.5],\n",
        "                                                           [0.5, 0.5, 0.5])])\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "valid_data = datasets.ImageFolder(data_dir + '/valid', transform=test_valid_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_valid_transforms)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# specify the image classes\n",
        "classes = train_loader.dataset.classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzWki5IEqmNk",
        "colab_type": "text"
      },
      "source": [
        "### Defining helper function to un-normalize an array and display an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwxaPcblEmX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "    img = img * 0.5 + 0.5  # unnormalize\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f5i9zf9quMg",
        "colab_type": "text"
      },
      "source": [
        "### Checking Transformation Results & Displaying Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-osebseW3IUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVNn5ETYtvs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of validation images\n",
        "dataiter = iter(valid_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efzVDa8g7QMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiRRez3ClNUD",
        "colab_type": "text"
      },
      "source": [
        "## Checking Array Size & Calculating Planned Layers Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs3v1E2SFvxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koOHBFleT8Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculating Planned Layers Output\n",
        "i = torch.randn(images.shape)\n",
        "print(i.shape)\n",
        "i = nn.Conv2d(3,16,3,1,1)(i)\n",
        "print(i.shape)\n",
        "i = nn.MaxPool2d(7,7)(i)\n",
        "print(i.shape)\n",
        "i = nn.Conv2d(16,32,3,1,1)(i)\n",
        "print(i.shape)\n",
        "i = nn.MaxPool2d(4,4)(i)\n",
        "print(i.shape)\n",
        "i = nn.Conv2d(32,64,3,1,1)(i)\n",
        "print(i.shape)\n",
        "i = nn.MaxPool2d(2,2)(i)\n",
        "print(i.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avD7o3BAlwDx",
        "colab_type": "text"
      },
      "source": [
        "## Defining CNN Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjsc00prt-e1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(7, 7)\n",
        "        self.pool2 = nn.MaxPool2d(4, 4)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        # linear layer (64 * 4 * 4 -> 512)\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 512)\n",
        "        # linear layer (512 -> 12)\n",
        "        self.fc2 = nn.Linear(512, 12)\n",
        "        # dropout layer (p=0.5)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # add sequence of convolutional and max pooling layers\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = self.pool3(F.relu(self.conv3(x)))\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 1st hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# create a complete CNN\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnSGrTb0mRoE",
        "colab_type": "text"
      },
      "source": [
        "## Defining Loss Function & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DbEoeRxt_Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-wVw8ldmW6t",
        "colab_type": "text"
      },
      "source": [
        "## Training The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfqdcUmEuA1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 200\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "running_train_loss = []\n",
        "running_valid_loss = []\n",
        "running_min_valid_loss = [] # running loss tracker\n",
        "\n",
        "from time import time\n",
        "end_time = 0\n",
        "cumulative_training_time = [] # training time tracker\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    # track training start time\n",
        "    start = time()\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    # train the model\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "    # calculates training duration in seconds\n",
        "    end = time() - start\n",
        "    end_time += end\n",
        "    cumulative_training_time.append(end_time)\n",
        "\n",
        "    # validate the model\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    running_train_loss.append(train_loss)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "    running_valid_loss.append(valid_loss)\n",
        "        \n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTraining Time: {:.3f}s'.format(\n",
        "        epoch, train_loss, valid_loss, end))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_zodiac.pt')\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "    # stop training when minimum validation loss doesn't improve after 11 iterations\n",
        "    running_min_valid_loss.append(valid_loss_min)\n",
        "    if len(running_min_valid_loss) > 9 and sum([i==j for i,j in zip(running_min_valid_loss[-11:-1],running_min_valid_loss[-10:])]) == 10:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omi2KtR9maMZ",
        "colab_type": "text"
      },
      "source": [
        "## Plotting The Training Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBtf4nzAf9uL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(17,5))\n",
        "x = range(1, len(running_train_loss)+1)\n",
        "\n",
        "ax.set_xlabel('epoch')\n",
        "ax.set_ylabel('loss')\n",
        "ax.plot(x, running_train_loss , label=\"Train Loss\")\n",
        "ax.plot(x, running_valid_loss, label=\"Validation Loss\")\n",
        "ax.legend()\n",
        "\n",
        "ax_2 = ax.twinx()\n",
        "ax_2.set_ylabel('time (s)', color = 'tab:green')\n",
        "ax_2.plot(x, cumulative_training_time, linestyle = '-.', color='green', alpha = 0.5)\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c-64OLbmsTl",
        "colab_type": "text"
      },
      "source": [
        "## Testing On New Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCGrJesLf4Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('model_zodiac.pt')) # Load the First Model with the Lowest Validation Loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g74tXQDPuC0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# track test loss\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(len(classes)))\n",
        "class_total = list(0. for i in range(len(classes)))\n",
        "\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for data, target in test_loader:\n",
        "    if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    # compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(len(classes)):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY-75mqtnB8A",
        "colab_type": "text"
      },
      "source": [
        "## Visualize Sample Test Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJKHmSZ_uFGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# move model inputs to cuda, if GPU available\n",
        "if train_on_gpu:\n",
        "    images = images.cuda()\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max(output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# convert images to numpy array for visualization\n",
        "images = images.numpy() if not train_on_gpu else images.cpu().numpy()\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t9REQszM2wE",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z3-tgJUNN6f",
        "colab_type": "text"
      },
      "source": [
        "## Transformations and Data Preparation\n",
        "\n",
        "All pretrained models in pytorch expects the images to be normalized with the following mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lglui5V9QFM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g4RoZbYRYUt",
        "colab_type": "text"
      },
      "source": [
        "With that being said we will need to re-transform the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skKMgvPPM01r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'signs'\n",
        "\n",
        "batch_size = 20\n",
        "\n",
        "# TODO: Define transforms for the training data and testing data\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=mean,\n",
        "                                                           std=std)])\n",
        "\n",
        "test_valid_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.RandomResizedCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=mean,\n",
        "                                                           std=std)])\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "valid_data = datasets.ImageFolder(data_dir + '/valid', transform=test_valid_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_valid_transforms)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# specify the image classes\n",
        "classes = train_loader.dataset.classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBXpUgaGRmX8",
        "colab_type": "text"
      },
      "source": [
        "Besides of that, we will also need to define a new helper function based on the new means and standard deviations. \n",
        "\n",
        "Formula for normalization is shown below.\n",
        "\n",
        "> `input[channel] = (input[channel] - mean[channel]) / std[channel]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qxFRvTpNmV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function to un-normalize and display an image\n",
        "def imshow(img, mean = mean, std = std):\n",
        "    for idx, (m ,s) in enumerate(zip(mean,std)):\n",
        "        img[idx] = img[idx] * s + m  # un-normalize array\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KidXPfyKUw1p",
        "colab_type": "text"
      },
      "source": [
        "Lets check whether our new imshow function gives the expected result or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA6R2qb-Uv7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvG5nNRUSCPE",
        "colab_type": "text"
      },
      "source": [
        "## Loading Pretrained Model\n",
        "\n",
        "The pretrained model we will be using for this classification is ResNet-18 model from [“Deep Residual Learning for Image Recognition”](https://arxiv.org/pdf/1512.03385.pdf). I do not have any particular reason for using this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwdMOk4aThKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVSgjrgC2S9H",
        "colab_type": "text"
      },
      "source": [
        "## Defining Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGNM7tCbwpbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "from collections import OrderedDict\n",
        "resnet_fc = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(512, 128)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('dropout1', nn.Dropout(p=0.5)),\n",
        "                          ('fc2', nn.Linear(128, 32)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('dropout2', nn.Dropout(p=0.5)),\n",
        "                          ('fc3', nn.Linear(32, 12)),\n",
        "                          ]))\n",
        "    \n",
        "model.fc = resnet_fc\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq-9EEUd4fmz",
        "colab_type": "text"
      },
      "source": [
        "## Defining Loss Function & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGsaHaKlx3it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2dyvLE14jvS",
        "colab_type": "text"
      },
      "source": [
        "## Training The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ9K6ocax_Gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 200\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "running_train_loss = []\n",
        "running_valid_loss = []\n",
        "running_min_valid_loss = [] # running loss tracker\n",
        "\n",
        "from time import time\n",
        "end_time = 0\n",
        "cumulative_training_time = [] # training time tracker\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    # track training start time\n",
        "    start = time()\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    # train the model\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "    # calculates training duration in seconds\n",
        "    end = time() - start\n",
        "    end_time += end\n",
        "    cumulative_training_time.append(end_time)\n",
        "\n",
        "    # validate the model\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    running_train_loss.append(train_loss)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "    running_valid_loss.append(valid_loss)\n",
        "        \n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTraining Time: {:.3f}s'.format(\n",
        "        epoch, train_loss, valid_loss, end))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_resnet_zodiac.pt')\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "    # stop training when minimum validation loss doesn't improve after 11 iterations\n",
        "    running_min_valid_loss.append(valid_loss_min)\n",
        "    if len(running_min_valid_loss) > 9 and sum([i==j for i,j in zip(running_min_valid_loss[-11:-1],running_min_valid_loss[-10:])]) == 10:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv7rnu6u4pc4",
        "colab_type": "text"
      },
      "source": [
        "## Plotting The Training Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGn57lLKyCWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(17,5))\n",
        "x = range(1, len(running_train_loss)+1)\n",
        "\n",
        "ax.set_xlabel('epoch')\n",
        "ax.set_ylabel('loss')\n",
        "ax.plot(x, running_train_loss , label=\"Train Loss\")\n",
        "ax.plot(x, running_valid_loss, label=\"Validation Loss\")\n",
        "ax.legend()\n",
        "\n",
        "ax_2 = ax.twinx()\n",
        "ax_2.set_ylabel('time (s)', color = 'tab:green')\n",
        "ax_2.plot(x, cumulative_training_time, linestyle = '-.', color='green', alpha = 0.5)\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gepbZk1k4uKE",
        "colab_type": "text"
      },
      "source": [
        "## Testing On New Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVqulRpu1kfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('model_resnet_zodiac.pt')) # Load the First Model with the Lowest Validation Loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK92aFMEyqEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# track test loss\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(len(classes)))\n",
        "class_total = list(0. for i in range(len(classes)))\n",
        "\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for data, target in test_loader:\n",
        "    if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    # compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(len(classes)):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh2vfQPF406C",
        "colab_type": "text"
      },
      "source": [
        "## Visualize Sample Test Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXFCV_I6yuNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# move model inputs to cuda, if GPU available\n",
        "if train_on_gpu:\n",
        "    images = images.cuda()\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max(output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# convert images to numpy array for visualization\n",
        "images = images.numpy() if not train_on_gpu else images.cpu().numpy()\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLQo_X0I38Ys",
        "colab_type": "text"
      },
      "source": [
        "# Download Final Model to Local Computer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s0x3AM2vtul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.files import download\n",
        "\n",
        "download('model_zodiac.pt')\n",
        "download('model_resnet_zodiac.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}